# Unit 4: Decision Trees

```{r}
Claims = read.csv("ClaimsData.csv")
```

```{r}
library(caTools)

set.seed(88)

spl = sample.split(Claims$bucket2009, SplitRatio = 0.6)

ClaimsTrain = subset(Claims, spl==TRUE)

ClaimsTest = subset(Claims, spl==FALSE)
```

```{r}
mean(ClaimsTrain$age)
```

```{r}
table(ClaimsTrain$diabetes)
104672/(104672+170131)
```

```{r}
PenaltyMatrix = matrix(c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1), byrow=TRUE, nrow=5)
as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix

sum(as.matrix(table(ClaimsTest$bucket2009, ClaimsTest$bucket2008))*PenaltyMatrix)/nrow(ClaimsTest)
```
```{r}
table(ClaimsTest$bucket2009)
(0*122978 + 2*34840 + 4*16390 + 6*7937 + 8*1057)/nrow(ClaimsTest) 
```

What proportion of people in this dataset voted in this election?
```{r}
gerber  <- read.csv('gerber.csv')
```
```{r}
table(gerber $voting)
108696/(108696+235388)
```

Which of the four "treatment groups" had the largest percentage of people who actually voted (voting = 1)?
```{r}
nrow(gerber[gerber$voting == 1 & gerber$hawthorne == 1,])/nrow(gerber)
nrow(gerber[gerber$voting == 1 & gerber$civicduty == 1,])/nrow(gerber)
nrow(gerber[gerber$voting == 1 & gerber$neighbors == 1,])/nrow(gerber)
nrow(gerber[gerber$voting == 1 & gerber$self == 1,])/nrow(gerber)

```
Build a logistic regression model for voting using the four treatment group variables as the independent variables (civicduty, hawthorne, self, and neighbors). Use all the data to build the model (DO NOT split the data into a training set and testing set). Which of the following coefficients are significant in the logistic regression model?
```{r}
log_fit <- glm(voting ~ hawthorne + civicduty + neighbors + self, family = "binomial", data = gerber)
summary(log_fit)
```

Using a threshold of 0.3, what is the accuracy of the logistic regression model?
```{r}
predict <- predict(log_fit, gerber, type = "response" )
```

```{r}
table(gerber$voting, predict >= .3)
(134513+51966)/nrow(gerber)
```

Using a threshold of 0.5, what is the accuracy of the logistic regression model?
```{r}
table(gerber$voting, predict >= .5)
(235388)/nrow(gerber)
```
Compare your previous two answers to the percentage of people who did not vote (the baseline accuracy) and compute the AUC of the model.
```{r}
library(ROCR)

ROCRpred = prediction(predict, gerber$voting)

as.numeric(performance(ROCRpred, "auc")@y.values)
```
We will now try out trees. Build a CART tree for voting using all data and the same four treatment variables we used before. Don't set the option method="class" - we are actually going to create a regression tree here. We are interested in building a tree to explore the fraction of people who vote, or the probability of voting. We’d like CART to split our groups if they have different probabilities of voting. If we used method=‘class’, CART would only split if one of the groups had a probability of voting above 50% and the other had a probability of voting less than 50% (since the predicted outcomes would be different). However, with regression trees, CART will split even if both groups have probability less than 50%.
```{r}
library(rpart)
library(rpart.plot)
CARTmodel = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber)
prp(CARTmodel)
```
```{r}
CARTmodel2 = rpart(voting ~ civicduty + hawthorne + self + neighbors, data=gerber, cp=0.0)
prp(CARTmodel2)
```

Make a new tree that includes the "sex" variable, again with cp = 0.0. Notice that sex appears as a split that is of secondary importance to the treatment group.
```{r}
CARTmodel3 <- rpart(voting ~ civicduty + hawthorne + self + neighbors + sex, data=gerber, cp = 0)
```

```{r}
prp(CARTmodel3)
```
n the "control" only tree, what is the absolute value of the difference in the predicted probability of voting between being in the control group versus being in a different group?
```{r}
control_tree <- rpart(voting ~ control, data = gerber, cp = 0)
c_tree2 <- rpart(voting ~control + sex, data = gerber, cp =0)
prp(control_tree, digits = 6)
prp(c_tree2, digits = 6)
```
```{r}
abs(.296638- .34)
```
Going back to logistic regression now, create a model using "sex" and "control".
```{r}
log_fit2 <- glm(voting ~control+sex, family = "binomial", data = gerber)
summary(log_fit2)
```

