
# Unit 5 Text Analytics

```{r}
#install.packages("tm")
library(tm)
#install.packages("SnowballC")
library(SnowballC)
```

```{r}

tweets = read.csv("tweets.csv", stringsAsFactors=FALSE)

str(tweets)


# Create dependent variable

tweets$Negative = as.factor(tweets$Avg <= -1)

table(tweets$Negative)
```

```{r}
# Create corpus
corpus = VCorpus(VectorSource(tweets$Tweet)) 


# Convert to lower-case

corpus = tm_map(corpus, content_transformer(tolower))



# Remove punctuation

corpus = tm_map(corpus, removePunctuation)



# Look at stop words 
#stopwords("english")[1:10]

# Remove stopwords and apple

corpus = tm_map(corpus, removeWords, c("apple", stopwords("english")))



# Stem document 

corpus = tm_map(corpus, stemDocument)



```

```{r}
# Create matrix

frequencies = DocumentTermMatrix(corpus)

#frequencies

# Look at matrix 

#inspect(frequencies[1000:1005,505:515])

# Check for sparsity

findFreqTerms(frequencies, lowfreq=100)
```

```{r}
library(rpart)
library(rpart.plot)
```
```{r}

library(caTools)
# Check for sparsity

findFreqTerms(frequencies, lowfreq=20)

# Remove sparse terms

sparse = removeSparseTerms(frequencies, 0.995)
sparse

# Convert to a data frame

tweetsSparse = as.data.frame(as.matrix(sparse))

# Make all variable names R-friendly

colnames(tweetsSparse) = make.names(colnames(tweetsSparse))

# Add dependent variable

tweetsSparse$Negative = tweets$Negative
set.seed(123)

split = sample.split(tweetsSparse$Negative, SplitRatio = 0.7)

trainSparse = subset(tweetsSparse, split==TRUE)
testSparse = subset(tweetsSparse, split==FALSE)
```

Build a confusion matrix (with a threshold of 0.5) and compute the accuracy of the model. What is the accuracy?
```{r}
log_model <- glm(Negative~., family = "binomial", data = trainSparse)
```
```{r}
predictions = predict(log_model, newdata=testSparse, type="response")
```

```{r}
table(testSparse$Negative, predictions > 0.5)
```
```{r}
(254+33)/(253+47+22+33)
```

How many cases of vandalism were detected in the history of this page?
```{r}
wiki <- read.csv('wiki.csv', stringsAsFactors=FALSE)
wiki$Vandal <- as.factor(wiki$Vandal)
```

```{r}
table(wiki$Vandal)
```
We will now use the bag of words approach to build a model. How many terms appear in dtmAdded?
```{r}
library(tm)
```

```{r}
corpus <- VCorpus(VectorSource(wiki$Added))
```

```{r}
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
```

```{r}
dtmAdded <- DocumentTermMatrix(corpus)
```
```{r}
length(dtmAdded$dimnames$Terms)
```

Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions, and call the new matrix sparseAdded. How many terms appear in sparseAdded?
```{r}
sparseAdded <- removeSparseTerms(dtmAdded, 0.997)
```
```{r}
sparseAdded
```

How many words are in the wordsRemoved data frame?
```{r}
wordsAdded <- as.data.frame(as.matrix(sparseAdded))
```

```{r}
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
```

```{r}
corpus <- VCorpus(VectorSource(wiki$Removed))
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stemDocument)
dtmRemoved <- DocumentTermMatrix(corpus)
sparseRemoved <- removeSparseTerms(dtmRemoved, 0.997)
```

```{r}
wordsRemoved <- as.data.frame(as.matrix(sparseRemoved))
```

```{r}
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
```

Combine the two data frames into a data frame called wikiWords
```{r}
wikiWords <- cbind(wordsAdded,wordsRemoved)
wikiWords$Vandal <- wiki$Vandal
```

What is the accuracy on the test set of a baseline method that always predicts "not vandalism" (the most frequent outcome)?
```{r}
library(caTools)
set.seed(123)
spl = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
```

```{r}
table(test$Vandal)
```
```{r}
618/(618+545)
```

Build a CART model to predict Vandal, using all of the other variables as independent variables.
```{r}
library(rpart.plot)
library(rpart)
tree <- rpart(Vandal~., data=train, method = "class")
```

What is the accuracy of the model on the test set, using a threshold of 0.5?
```{r}
pred <- predict(tree, newdata = test,type = "class" )
```

```{r}
#table(pred[,2]>=0.5, test$Vandal)
table(test$Vandal, pred)
```
```{r}
(618+12)/(618+533+12)
```
```{r}
prp(tree)
```


```{r}
wikiWords2 = wikiWords
```

```{r}
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
```

Make a new column in wikiWords2 that is 1 if "http" was in Added
ased on this new column, how many revisions added a link?
```{r}
table(wikiWords2$HTTP)
```
In problem 1.5, you computed a vector called "spl" that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets:
Then create a new CART model using this new variable as one of the independent variables.

What is the new accuracy of the CART model on the test set, using a threshold of 0.5?
```{r}
wikiTrain2 = subset(wikiWords2, spl==TRUE)

wikiTest2 = subset(wikiWords2, spl==FALSE)
```

```{r}
tree2 <- rpart(Vandal~., data = wikiTrain2, method = "class")
```

```{r}
pred2 <- predict(tree2, newdata = wikiTest2, type = "class")
```
```{r}
table(pred2, wikiTest2$Vandal)
```
```{r}
(609+57)/(609+488+9+57)
```

Sum the rows of dtmAdded and dtmRemoved and add them as new variables in your data frame wikiWords2
```{r}
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))

wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
```

What is the average number of words added?
```{r}
mean(wikiWords2$NumWordsAdded)
```

In problem 1.5, you computed a vector called "spl" that identified the observations to put in the training and testing sets. Use that variable (do not recompute it with sample.split) to make new training and testing sets with wikiWords2. Create the CART model again (using the training set and the default parameters).

What is the new accuracy of the CART model on the test set?
```{r}
wikiTrain3 = subset(wikiWords2, spl==TRUE)

wikiTest3 = subset(wikiWords2, spl==FALSE)
```

```{r}
tree3 <- rpart(Vandal~., data = wikiTrain3, method= "class")
pred3 <- predict(tree3, newdata = wikiTest3, type = "class")
table(pred3, wikiTest3$Vandal)
                  
```
```{r}
(514+248)/(514+297+104+248)
```

We have two pieces of "metadata" (data about data) that we haven't yet used. Make a copy of wikiWords2, and call it wikiWords3:
```{r}
wikiWords3 = wikiWords2
```
Then add the two original variables Minor and Loggedin to this new data frame
```{r}
wikiWords3$Minor = wiki$Minor

wikiWords3$Loggedin = wiki$Loggedin
```
Build a CART model using all the training data. What is the accuracy of the model on the test set?
```{r}
wikiTrain3 = subset(wikiWords3, spl==TRUE)

wikiTest3 = subset(wikiWords3, spl==FALSE)
```

```{r}
tree4 <- rpart(Vandal~., data = wikiTrain3, method="class")
pred4 <- predict(tree4, newdata = wikiTest3, type = "class")
table(pred4, wikiTest3$Vandal)
```
```{r}
(595+241)/nrow(wikiTest3)
```
```{r}
prp(tree4)
```

