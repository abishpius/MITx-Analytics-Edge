# Unit 2 Linear Regression

```{r}
setwd("~/R_Projects/Analytics_Edge/Unit2_Linear Regression")
```

```{r}
data <- read.csv('wine.csv')
```

```{r}
lin_model <- lm(Price ~ HarvestRain + WinterRain, data = data)
```

```{r}
summary(lin_model)
```

```{r}
summary(lm(Price ~ HarvestRain + WinterRain, data = data))
```

```{r}
cor(data$HarvestRain, data$WinterRain)
```

Assignment 2

```{r}
climate <- read.csv("climate_change.csv")
```

We are interested in how changes in these variables affect future temperatures, as well as how well these variables explain temperature changes so far. To do this, first read the dataset climate_change.csv into R.

Then, split the data into a training set, consisting of all the observations up to and including 2006, and a testing set consisting of the remaining years (hint: use subset). A training set refers to the data that will be used to build the model (this is the data we give to the lm() function), and a testing set refers to the data we will use to test our predictive ability.

Next, build a linear regression model to predict the dependent variable Temp, using MEI, CO2, CH4, N2O, CFC.11, CFC.12, TSI, and Aerosols as independent variables (Year and Month should NOT be used in the model). Use the training set to build the model.

Enter the model R2 (the "Multiple R-squared" value):
```{r}
train <- subset(climate, Year <= 2006)
```
Which variables are significant in the model? We will consider a variable signficant only if the p-value is below 0.05.
```{r}
model1 <- lm(Temp ~ MEI+ CO2+ CH4+ N2O+ CFC.11+ CFC.12+TSI+Aerosols, data = train)
summary(model1)
```
```{r}
cor(train)
```
Given that the correlations are so high, let us focus on the N2O variable and build a model with only MEI, TSI, Aerosols and N2O as independent variables. Remember to use the training set to build the model.

Enter the coefficient of N2O in this reduced model:
Enter the model R2:

```{r}
model2 <- lm(Temp ~ MEI+ TSI+ Aerosols +N2O,data = train)
summary(model2)
```

Enter the R2 value of the model produced by the step function:
```{r}
model3 <- step(model1)
summary(model3)
```
```{r}
test <- subset(climate, Year > 2006)
predictions <- predict(model3, test)
```

```{r}
SSE <- sum((predictions - test$Temp)^2)
SST <- sum((mean(train$Temp) - test$Temp)^2)
R2 <- 1- SSE/SST
```

Load the training and testing sets using the read.csv() function, and save them as variables with the names pisaTrain and pisaTest.

How many students are there in the training set?
```{r}
pisa_train <- read.csv("pisa2009train.csv")
pisa_test <- read.csv("pisa2009test.csv")
```
```{r}
nrow(pisa_train)
```
Using tapply() on pisaTrain, what is the average reading test score of males?
```{r}
tapply(pisa_train$readingScore, pisa_train$male, mean)
```
Which variables are missing data in at least one observation in the training set? Select all that apply.
```{r}
summary(pisa_train)
```
Linear regression discards observations with missing data, so we will remove all such observations from the training and testing sets. Later in the course, we will learn about imputation, which deals with missing data by filling in missing values with plausible information.

Type the following commands into your R console to remove observations with any missing value from pisaTrain and pisaTest:

```{r}
pisaTrain = na.omit(pisa_train)

pisaTest = na.omit(pisa_test)
```


How many observations are now in the training set?
```{r}
nrow(pisaTrain)
nrow(pisaTest)
```

Factor variables are variables that take on a discrete set of values, like the "Region" variable in the WHO dataset from the second lecture of Unit 1. This is an unordered factor because there isn't any natural ordering between the levels. An ordered factor has a natural ordering between the levels (an example would be the classifications "large," "medium," and "small").

Which of the following variables is an unordered factor with at least 3 levels? (Select all that apply.)
```{r}
str(pisaTrain)
```
Which binary variables will be included in the regression model? (Select all that apply.)
```{r}
unique(pisaTrain$raceeth)
```

Because the race variable takes on text values, it was loaded as a factor variable when we read in the dataset with read.csv() -- you can see this when you run str(pisaTrain) or str(pisaTest). However, by default R selects the first level alphabetically ("American Indian/Alaska Native") as the reference level of our factor instead of the most common level ("White"). Set the reference level of the factor by typing the following two lines in your R console:

```{r}
pisaTrain$raceeth <- as.factor(pisaTrain$raceeth)
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")

pisaTest$raceeth <- as.factor(pisaTest$raceeth)
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
```


Now, build a linear regression model (call it lmScore) using the training set to predict readingScore using all the remaining variables.

It would be time-consuming to type all the variables, but R provides the shorthand notation "readingScore ~ ." to mean "predict readingScore using all the other variables in the data frame." The period is used to replace listing out all of the independent variables. As an example, if your dependent variable is called "Y", your independent variables are called "X1", "X2", and "X3", and your training data set is called "Train", instead of the regular notation:

LinReg = lm(Y ~ X1 + X2 + X3, data = Train)

You would use the following command to build your model:

LinReg = lm(Y ~ ., data = Train)

What is the Multiple R-squared value of lmScore on the training set?
```{r}
model4 <- lm(readingScore ~., data = pisaTrain)
summary(model4)
```
What is the training-set root-mean squared error (RMSE) of lmScore?
```{r}
predict_train <- predict(model4, pisaTrain)
SSE_train <- sum((predict_train - pisaTrain$readingScore)^2)
RMSE <- sqrt(SSE_train/nrow(pisaTrain))
```
Consider two students A and B. They have all variable values the same, except that student A is in grade 11 and student B is in grade 9. What is the predicted reading score of student A minus the predicted reading score of student B?
```{r}
2.937399+29.542707*11 - (2.937399+29.542707*9)
```
What is the range between the maximum and minimum predicted reading score on the test set?
```{r}
predict_test <- predict(model4, pisaTest)
max(predict_test) - min(predict_test)
```

What is the sum of squared errors (SSE) of lmScore on the testing set?hat is the root-mean squared error (RMSE) of lmScore on the testing set?
```{r}
SSE_test <- sum((predict_test - pisaTest$readingScore)^2)
RMSE_test <- sqrt(SSE_test/nrow(pisaTest))
```

What is the predicted test score used in the baseline model? Remember to compute this value using the training set and not the test set.
```{r}
mean(pisaTrain$readingScore)
```
What is the sum of squared errors of the baseline model on the testing set?
```{r}
SST_test <- sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
SST_test
```

What is the test-set R-squared value of lmScore?
```{r}
R_test <- 1-SSE_test/SST_test
R_test
```

